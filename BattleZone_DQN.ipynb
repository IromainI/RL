{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "855158c8-b29d-4b4f-8f64-9835680bd5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d64709a-4eea-4777-a96c-bbb96b878f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(18)\n",
      "Observation Space: Box(0, 255, (210, 160, 3), uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▌                        | 1/47 [01:08<52:08, 68.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/47, Total Reward: 5000.0, Epsilon: 0.9326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|█                        | 2/47 [01:55<41:53, 55.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2/47, Total Reward: 2000.0, Epsilon: 0.8886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|█▌                       | 3/47 [02:40<37:19, 50.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3/47, Total Reward: 1000.0, Epsilon: 0.8484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   9%|██▏                      | 4/47 [03:54<43:02, 60.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4/47, Total Reward: 0.0, Epsilon: 0.7870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  11%|██▋                      | 5/47 [05:02<44:07, 63.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5/47, Total Reward: 3000.0, Epsilon: 0.7354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  13%|███▏                     | 6/47 [05:57<41:10, 60.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6/47, Total Reward: 1000.0, Epsilon: 0.6968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  15%|███▋                     | 7/47 [06:57<40:09, 60.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7/47, Total Reward: 2000.0, Epsilon: 0.6581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  17%|████▎                    | 8/47 [08:21<44:06, 67.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8/47, Total Reward: 0.0, Epsilon: 0.6084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  19%|████▊                    | 9/47 [09:10<39:08, 61.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9/47, Total Reward: 0.0, Epsilon: 0.5812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  21%|█████                   | 10/47 [09:54<34:45, 56.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10/47, Total Reward: 1000.0, Epsilon: 0.5576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  23%|█████▌                  | 11/47 [10:45<32:52, 54.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11/47, Total Reward: 1000.0, Epsilon: 0.5313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  26%|██████▏                 | 12/47 [11:45<32:47, 56.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12/47, Total Reward: 1000.0, Epsilon: 0.5026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  28%|██████▋                 | 13/47 [13:16<37:46, 66.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13/47, Total Reward: 2000.0, Epsilon: 0.4628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  30%|███████▏                | 14/47 [14:14<35:21, 64.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14/47, Total Reward: 3000.0, Epsilon: 0.4387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  32%|███████▋                | 15/47 [14:55<30:32, 57.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15/47, Total Reward: 2000.0, Epsilon: 0.4224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  34%|████████▏               | 16/47 [16:17<33:19, 64.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16/47, Total Reward: 2000.0, Epsilon: 0.3926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  36%|████████▋               | 17/47 [17:43<35:27, 70.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17/47, Total Reward: 4000.0, Epsilon: 0.3630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  38%|████████▊              | 18/47 [20:45<50:32, 104.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18/47, Total Reward: 11000.0, Epsilon: 0.3080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  40%|█████████▋              | 19/47 [21:24<39:33, 84.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19/47, Total Reward: 1000.0, Epsilon: 0.2975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  43%|██████████▏             | 20/47 [22:47<37:56, 84.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20/47, Total Reward: 1000.0, Epsilon: 0.2762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  45%|██████████▋             | 21/47 [23:38<32:13, 74.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 21/47, Total Reward: 1000.0, Epsilon: 0.2640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  47%|███████████▏            | 22/47 [24:50<30:36, 73.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 22/47, Total Reward: 8000.0, Epsilon: 0.2474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  49%|███████████▋            | 23/47 [25:51<27:53, 69.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 23/47, Total Reward: 1000.0, Epsilon: 0.2342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  51%|████████████▎           | 24/47 [27:11<27:52, 72.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 24/47, Total Reward: 5000.0, Epsilon: 0.2180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  53%|████████████▊           | 25/47 [29:30<33:58, 92.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25/47, Total Reward: 2000.0, Epsilon: 0.1906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  55%|█████████████▎          | 26/47 [31:03<32:26, 92.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 26/47, Total Reward: 3000.0, Epsilon: 0.1751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  57%|█████████████▊          | 27/47 [32:13<28:41, 86.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 27/47, Total Reward: 1000.0, Epsilon: 0.1642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  60%|██████████████▎         | 28/47 [33:25<25:53, 81.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 28/47, Total Reward: 14000.0, Epsilon: 0.1538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  62%|██████████████▊         | 29/47 [35:38<29:09, 97.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 29/47, Total Reward: 6000.0, Epsilon: 0.1364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  64%|███████████████▎        | 30/47 [36:41<24:39, 87.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 30/47, Total Reward: 1000.0, Epsilon: 0.1287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  66%|███████████████▊        | 31/47 [37:30<20:06, 75.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 31/47, Total Reward: 2000.0, Epsilon: 0.1231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  68%|████████████████▎       | 32/47 [38:28<17:35, 70.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 32/47, Total Reward: 1000.0, Epsilon: 0.1166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  70%|████████████████▊       | 33/47 [39:36<16:13, 69.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 33/47, Total Reward: 5000.0, Epsilon: 0.1096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  72%|█████████████████▎      | 34/47 [41:31<18:02, 83.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 34/47, Total Reward: 0.0, Epsilon: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  74%|█████████████████▊      | 35/47 [42:24<14:48, 74.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 35/47, Total Reward: 2000.0, Epsilon: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  77%|██████████████████▍     | 36/47 [43:26<12:54, 70.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 36/47, Total Reward: 2000.0, Epsilon: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  79%|██████████████████▉     | 37/47 [44:31<11:29, 68.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 37/47, Total Reward: 1000.0, Epsilon: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  81%|███████████████████▍    | 38/47 [45:17<09:18, 62.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 38/47, Total Reward: 1000.0, Epsilon: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  83%|███████████████████▉    | 39/47 [46:32<08:47, 65.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 39/47, Total Reward: 0.0, Epsilon: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  85%|████████████████████▍   | 40/47 [47:38<07:42, 66.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 40/47, Total Reward: 1000.0, Epsilon: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  87%|████████████████████▉   | 41/47 [49:08<07:19, 73.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 41/47, Total Reward: 2000.0, Epsilon: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  89%|█████████████████████▍  | 42/47 [50:15<05:56, 71.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 42/47, Total Reward: 0.0, Epsilon: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  91%|█████████████████████▉  | 43/47 [50:59<04:12, 63.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 43/47, Total Reward: 2000.0, Epsilon: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  94%|██████████████████████▍ | 44/47 [51:57<03:04, 61.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 44/47, Total Reward: 2000.0, Epsilon: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  96%|██████████████████████▉ | 45/47 [53:39<02:27, 73.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 45/47, Total Reward: 2000.0, Epsilon: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  98%|███████████████████████▍| 46/47 [55:41<01:28, 88.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 46/47, Total Reward: 5000.0, Epsilon: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████████| 47/47 [56:25<00:00, 72.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 47/47, Total Reward: 2000.0, Epsilon: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"ALE/BattleZone-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self._calculate_conv_output(input_shape), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "    \n",
    "    def _calculate_conv_output(self, shape):\n",
    "        with torch.no_grad():\n",
    "            o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.99995\n",
    "        self.gamma = 0.95\n",
    "        self.learning_rate = 0.0001\n",
    "        self.batch_size = 32\n",
    "        self.memory_capacity = 30000\n",
    "        \n",
    "        self.policy_net = DQN(input_shape, num_actions).to(self.device)\n",
    "        self.target_net = DQN(input_shape, num_actions).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.memory = ReplayBuffer(self.memory_capacity)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.num_actions - 1)\n",
    "        else:\n",
    "            state = np.transpose(state, (2, 0, 1))  # Convert (H, W, C) -> (C, H, W)\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            q_values = self.policy_net(state)\n",
    "            return q_values.max(1)[1].item()\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        states = np.transpose(np.array(states), (0, 3, 1, 2))\n",
    "        next_states = np.transpose(np.array(next_states), (0, 3, 1, 2))\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.tensor(rewards).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        q_values = self.policy_net(states).gather(1, actions)\n",
    "        next_q_values = self.target_net(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "\n",
    "        loss = self.loss_fn(q_values.squeeze(), target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        rewards_history.append(total_reward)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "num_episodes = 47\n",
    "max_steps_per_episode = 10000\n",
    "input_shape = (3, 210, 160)\n",
    "num_actions = env.action_space.n\n",
    "rewards_history = []\n",
    "\n",
    "agent = DQNAgent(input_shape, num_actions)\n",
    "\n",
    "for episode in tqdm(range(num_episodes), desc=\"Training Progress\"):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        agent.memory.push(state, action, reward, next_state, done)\n",
    "        agent.update()\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if episode % 5 == 0:\n",
    "        agent.update_target_network()\n",
    "    \n",
    "    print(f\"Episode {episode+1}/{num_episodes}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "torch.save(agent.policy_net.state_dict(), \"dqn_policy_weights.pth\")\n",
    "torch.save(agent.target_net.state_dict(), \"dqn_target_weights.pth\")\n",
    "\n",
    "with open(\"rewards_history.json\", \"w\") as f:\n",
    "    json.dump(rewards_history, f)\n",
    "\n",
    "def record_video(env, agent, out_path, fps=30):\n",
    "    frames = []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        \n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        state = next_state\n",
    "    \n",
    "    imageio.mimsave(out_path, frames, fps=fps)\n",
    "\n",
    "record_video(env, agent, \"/Users/romain/Rein/battlezone_play.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "736c9a69-ade3-49ab-a9e3-5024b540f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rewards_history.json\", \"w\") as f:\n",
    "    json.dump(rewards_history, f)\n",
    "torch.save(agent.policy_net.state_dict(), \"dqn_policy_weights.pth\")\n",
    "torch.save(agent.target_net.state_dict(), \"dqn_target_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf56b9c5-565a-4ede-8f38-2ef095455220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv.0.weight: torch.Size([32, 3, 8, 8])\n",
      "tensor([[[[ 0.0166, -0.0155,  0.0329,  ..., -0.0142, -0.0528,  0.0652],\n",
      "          [-0.0304, -0.0211,  0.0529,  ..., -0.0389,  0.0263,  0.0414],\n",
      "          [-0.0318, -0.0312, -0.0301,  ..., -0.1082, -0.1115, -0.0698],\n",
      "          ...,\n",
      "          [-0.0299,  0.0591,  0.0680,  ..., -0.0145, -0.0033,  0.0792],\n",
      "          [-0.0555, -0.0620,  0.0859,  ..., -0.0184, -0.1053,  0.0798],\n",
      "          [-0.0714, -0.1015,  0.0053,  ..., -0.0668, -0.0589, -0.0394]],\n",
      "\n",
      "         [[-0.0657,  0.0215,  0.0217,  ...,  0.0607,  0.0148, -0.0263],\n",
      "          [ 0.0151,  0.0118, -0.0085,  ..., -0.0392, -0.0620,  0.0486],\n",
      "          [ 0.0149,  0.0315,  0.1018,  ...,  0.0433, -0.0978, -0.0556],\n",
      "          ...,\n",
      "          [ 0.0769,  0.0185,  0.0964,  ...,  0.0683, -0.0710,  0.0325],\n",
      "          [ 0.0281,  0.0636,  0.0039,  ...,  0.0331, -0.0523,  0.0727],\n",
      "          [-0.0264, -0.0233,  0.0522,  ...,  0.0415,  0.0164, -0.0671]],\n",
      "\n",
      "         [[-0.0321,  0.0774,  0.0380,  ..., -0.0131,  0.0013,  0.0356],\n",
      "          [ 0.0160, -0.0691,  0.0091,  ...,  0.0245, -0.0466,  0.0165],\n",
      "          [-0.0573,  0.0250, -0.0247,  ...,  0.0197,  0.0351, -0.0381],\n",
      "          ...,\n",
      "          [-0.0183,  0.0839,  0.0451,  ..., -0.0337,  0.0031,  0.0437],\n",
      "          [ 0.0286, -0.0493,  0.0568,  ...,  0.0140, -0.0197,  0.0224],\n",
      "          [-0.0216,  0.0022,  0.0612,  ...,  0.0016, -0.0640, -0.0327]]],\n",
      "\n",
      "\n",
      "        [[[-0.0091, -0.0598,  0.0436,  ..., -0.0694, -0.0408, -0.0453],\n",
      "          [-0.0059,  0.0429, -0.0352,  ...,  0.0257, -0.0262, -0.1026],\n",
      "          [ 0.0181,  0.0057, -0.0690,  ..., -0.1316,  0.0872, -0.0155],\n",
      "          ...,\n",
      "          [-0.0385,  0.0282, -0.0341,  ...,  0.0390,  0.0490, -0.0469],\n",
      "          [ 0.0196, -0.0533,  0.0902,  ...,  0.0377, -0.0465,  0.0084],\n",
      "          [ 0.0300, -0.0836,  0.0367,  ..., -0.0662, -0.0322,  0.0458]],\n",
      "\n",
      "         [[-0.1197, -0.0953,  0.0351,  ..., -0.0061,  0.0826,  0.0027],\n",
      "          [-0.0398,  0.0153,  0.0233,  ...,  0.0834, -0.0577, -0.0377],\n",
      "          [-0.0807,  0.0479,  0.0140,  ..., -0.1205,  0.0979, -0.0717],\n",
      "          ...,\n",
      "          [-0.0049,  0.0787, -0.0169,  ..., -0.0345,  0.0444, -0.0815],\n",
      "          [ 0.0253, -0.0415,  0.0236,  ..., -0.0799, -0.0240, -0.0116],\n",
      "          [ 0.0380, -0.0525,  0.0080,  ..., -0.0661,  0.0244, -0.0199]],\n",
      "\n",
      "         [[-0.0505, -0.0006,  0.0184,  ...,  0.0096,  0.1112,  0.0150],\n",
      "          [ 0.0010,  0.0798, -0.0787,  ...,  0.0277, -0.0567,  0.0499],\n",
      "          [-0.0610,  0.0740, -0.0501,  ..., -0.0521,  0.0783,  0.0672],\n",
      "          ...,\n",
      "          [-0.0169,  0.0235, -0.0393,  ...,  0.0206,  0.0325, -0.1126],\n",
      "          [-0.0497, -0.0725,  0.0556,  ..., -0.0272, -0.0592, -0.0757],\n",
      "          [-0.0431, -0.0606, -0.0099,  ..., -0.0224,  0.0386, -0.0411]]],\n",
      "\n",
      "\n",
      "        [[[-0.0426, -0.0050, -0.0757,  ...,  0.0175,  0.0800,  0.0791],\n",
      "          [-0.0835,  0.0310, -0.0048,  ...,  0.0319, -0.0789, -0.1329],\n",
      "          [ 0.0545, -0.0021, -0.0099,  ...,  0.0471, -0.0274,  0.0483],\n",
      "          ...,\n",
      "          [ 0.0565,  0.0488, -0.0628,  ...,  0.0094,  0.0014, -0.0724],\n",
      "          [-0.0884,  0.0172,  0.0147,  ...,  0.0559, -0.0162, -0.0785],\n",
      "          [ 0.0463, -0.0662, -0.0157,  ...,  0.0115, -0.0630, -0.0845]],\n",
      "\n",
      "         [[ 0.0391, -0.0102, -0.0119,  ..., -0.0817, -0.0013, -0.0169],\n",
      "          [-0.0166,  0.0081, -0.0795,  ...,  0.0544, -0.0559, -0.0375],\n",
      "          [ 0.0282, -0.0598, -0.0595,  ..., -0.0544,  0.0317,  0.0487],\n",
      "          ...,\n",
      "          [ 0.0347, -0.0304, -0.0449,  ...,  0.0059,  0.0191, -0.0083],\n",
      "          [ 0.0424,  0.0348,  0.1008,  ..., -0.0192, -0.0182,  0.0284],\n",
      "          [-0.0334,  0.0213,  0.0021,  ...,  0.0328, -0.0331,  0.0161]],\n",
      "\n",
      "         [[-0.0433,  0.0228, -0.0086,  ..., -0.1000,  0.0660,  0.0731],\n",
      "          [-0.0306, -0.0393, -0.0464,  ..., -0.0312, -0.0385, -0.0274],\n",
      "          [-0.0055, -0.0054, -0.0429,  ...,  0.0204, -0.0171,  0.0259],\n",
      "          ...,\n",
      "          [-0.0481,  0.0676, -0.0187,  ...,  0.0509, -0.0275, -0.0158],\n",
      "          [-0.0585, -0.0842, -0.0090,  ..., -0.0636, -0.0348, -0.0708],\n",
      "          [-0.0232, -0.0593,  0.0538,  ..., -0.0027, -0.0579,  0.0406]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0098,  0.0109, -0.0443,  ...,  0.0680,  0.1030, -0.0173],\n",
      "          [-0.0207,  0.0515, -0.0157,  ...,  0.0238, -0.0295, -0.0596],\n",
      "          [ 0.0194,  0.0344,  0.0873,  ..., -0.0160, -0.0261,  0.0562],\n",
      "          ...,\n",
      "          [-0.0065,  0.0290, -0.0436,  ...,  0.0113, -0.0428, -0.1218],\n",
      "          [-0.0277,  0.0294, -0.0372,  ...,  0.0205, -0.0833, -0.0554],\n",
      "          [ 0.0969, -0.1223,  0.0141,  ..., -0.0046,  0.0255,  0.0229]],\n",
      "\n",
      "         [[-0.0639,  0.0527,  0.0617,  ...,  0.0285,  0.0274, -0.1176],\n",
      "          [ 0.0145,  0.0580,  0.0168,  ...,  0.0023,  0.0164, -0.0791],\n",
      "          [ 0.0493,  0.0110,  0.0521,  ..., -0.0646, -0.0302,  0.0056],\n",
      "          ...,\n",
      "          [-0.0438,  0.0521,  0.0087,  ...,  0.0265, -0.0837, -0.1520],\n",
      "          [ 0.0444, -0.0181,  0.0139,  ..., -0.0512, -0.0832, -0.0453],\n",
      "          [ 0.0298, -0.0494, -0.0142,  ..., -0.0228, -0.0738,  0.0407]],\n",
      "\n",
      "         [[ 0.0118, -0.0500, -0.0182,  ...,  0.0171,  0.1005, -0.1131],\n",
      "          [-0.0680,  0.0476,  0.0635,  ..., -0.1038,  0.0801, -0.0748],\n",
      "          [-0.0081,  0.0038,  0.0207,  ..., -0.0918, -0.0849, -0.0055],\n",
      "          ...,\n",
      "          [ 0.0615,  0.0645,  0.0216,  ..., -0.0577, -0.0934, -0.0863],\n",
      "          [ 0.0709,  0.0691,  0.0049,  ...,  0.0569, -0.1447,  0.0237],\n",
      "          [ 0.0622, -0.0972, -0.0292,  ...,  0.0629, -0.0762, -0.0073]]],\n",
      "\n",
      "\n",
      "        [[[-0.0090,  0.0127,  0.0014,  ..., -0.0324, -0.0355,  0.0548],\n",
      "          [-0.1616,  0.0704, -0.0556,  ..., -0.0516,  0.0434,  0.0715],\n",
      "          [ 0.0399, -0.0269,  0.0406,  ..., -0.0233,  0.0340,  0.0303],\n",
      "          ...,\n",
      "          [-0.0332, -0.0748,  0.1220,  ...,  0.0593, -0.0618,  0.0020],\n",
      "          [-0.0142, -0.0605,  0.0331,  ..., -0.0046, -0.1010, -0.0052],\n",
      "          [ 0.0618, -0.0401,  0.0002,  ...,  0.0650, -0.1033,  0.0706]],\n",
      "\n",
      "         [[-0.0273, -0.0682, -0.0348,  ...,  0.0059,  0.0339,  0.0200],\n",
      "          [-0.0986,  0.0352,  0.0296,  ...,  0.0161, -0.0057, -0.0650],\n",
      "          [-0.0320,  0.0094,  0.1087,  ..., -0.1190, -0.0175, -0.0151],\n",
      "          ...,\n",
      "          [ 0.0024,  0.0390,  0.0776,  ..., -0.0177, -0.0509,  0.0484],\n",
      "          [-0.0940, -0.0248,  0.0278,  ...,  0.0527, -0.0479, -0.0536],\n",
      "          [ 0.0260, -0.0086,  0.0469,  ..., -0.0252, -0.0944,  0.0413]],\n",
      "\n",
      "         [[-0.0823, -0.0154, -0.0478,  ..., -0.0706, -0.0217, -0.0198],\n",
      "          [-0.0967, -0.0308, -0.0408,  ..., -0.0754,  0.0655,  0.0406],\n",
      "          [ 0.0882,  0.0668,  0.1203,  ..., -0.0361,  0.0076,  0.0159],\n",
      "          ...,\n",
      "          [-0.0391,  0.0746,  0.1449,  ..., -0.0564, -0.0348,  0.0119],\n",
      "          [-0.0251, -0.0800,  0.0634,  ..., -0.0276, -0.0396,  0.0397],\n",
      "          [ 0.0157, -0.0417, -0.0555,  ..., -0.0392, -0.1053,  0.0572]]],\n",
      "\n",
      "\n",
      "        [[[-0.0817,  0.0413,  0.0205,  ..., -0.0240,  0.0141, -0.0064],\n",
      "          [ 0.0366,  0.0062,  0.0285,  ...,  0.0665,  0.0331, -0.1315],\n",
      "          [ 0.0551, -0.0022, -0.0286,  ..., -0.0349, -0.0651, -0.0182],\n",
      "          ...,\n",
      "          [ 0.0467,  0.0354, -0.0597,  ...,  0.0631,  0.0605, -0.0051],\n",
      "          [-0.0040, -0.0640,  0.0704,  ..., -0.0576,  0.0702, -0.0429],\n",
      "          [ 0.0500,  0.0361,  0.0844,  ..., -0.0311,  0.0776, -0.0562]],\n",
      "\n",
      "         [[ 0.0572,  0.0821, -0.0577,  ...,  0.0224,  0.0399,  0.0052],\n",
      "          [-0.0236,  0.0157,  0.0070,  ..., -0.0157,  0.0411, -0.1179],\n",
      "          [ 0.0933, -0.0489, -0.0651,  ..., -0.0767,  0.0494, -0.0265],\n",
      "          ...,\n",
      "          [-0.0530, -0.0067,  0.0291,  ...,  0.0231,  0.0673, -0.0194],\n",
      "          [-0.0610, -0.0422,  0.0684,  ...,  0.0429,  0.0640,  0.0024],\n",
      "          [ 0.0215,  0.0720, -0.0140,  ..., -0.0864, -0.0274,  0.0338]],\n",
      "\n",
      "         [[ 0.0803,  0.0264, -0.0265,  ..., -0.0335,  0.0305,  0.0264],\n",
      "          [-0.0169, -0.0140, -0.0690,  ...,  0.0150, -0.0252, -0.0300],\n",
      "          [-0.0278, -0.0251,  0.0396,  ..., -0.0519,  0.0234,  0.0077],\n",
      "          ...,\n",
      "          [ 0.0638,  0.0092,  0.0076,  ...,  0.0484,  0.1035,  0.0319],\n",
      "          [ 0.0182,  0.0568,  0.0105,  ...,  0.0629,  0.0895, -0.0751],\n",
      "          [-0.0705,  0.0239, -0.0088,  ..., -0.0218,  0.0357, -0.0872]]]])\n"
     ]
    }
   ],
   "source": [
    "policy_weights = torch.load(\"dqn_policy_weights.pth\")\n",
    "\n",
    "for name, weight in policy_weights.items():\n",
    "    print(f\"{name}: {weight.shape}\")\n",
    "    print(weight)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7d8a47e-1b6a-4c30-8a41-0c38ada8db9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values for the input: tensor([[0.2117, 0.2958, 0.1526, 0.1713, 0.3313, 0.2374, 0.2093, 0.2602, 0.1805,\n",
      "         0.2498, 0.1894, 0.2541, 0.2192, 0.3707, 0.4587, 0.3392, 0.2776, 0.4882]])\n",
      "Best action based on the Q-values: 17\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Задаём параметры модели\n",
    "input_shape = (3, 210, 160)  # Размер входных данных (например, для RGB-изображения)\n",
    "num_actions = 18  # Количество возможных действий (зависит от вашей среды)\n",
    "\n",
    "# Загружаем модель (предположим, что DQN определена в вашем коде)\n",
    "model = DQN(input_shape, num_actions)\n",
    "\n",
    "# Загружаем веса в модель\n",
    "model.load_state_dict(torch.load('dqn_policy_weights.pth'))\n",
    "model.eval()  # Устанавливаем модель в режим оценки (выключает dropout и batch normalization)\n",
    "with torch.no_grad():  # Отключаем градиенты, так как они нам не нужны для предсказаний\n",
    "    q_values = model(random_input_tensor)  # Получаем Q-значения для всех возможных действий\n",
    "\n",
    "# Выводим результат\n",
    "print(\"Q-values for the input:\", q_values)\n",
    "\n",
    "# Получаем индекс действия с максимальным Q-значением\n",
    "best_action = q_values.argmax().item()\n",
    "print(f\"Best action based on the Q-values: {best_action}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465d6cd-e8a1-45c8-9126-80c52dd0ff65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
